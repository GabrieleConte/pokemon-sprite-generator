# Configuration for training the Pokemon sprite generator

# Experiment settings
experiment_dir: "/Users/gabrieleconte/Developer/pokemon-sprite-generator/experiments"

# Model configuration
model:
  bert_model: "prajjwal1/bert-mini"
  text_embedding_dim: 256
  latent_dim: 8  # 8 channels for 27x27 latent space
  
  # U-Net parameters
  time_emb_dim: 128
  num_heads: 8
  
  # Diffusion parameters
  num_timesteps: 1000
  beta_start: 0.0001
  beta_end: 0.02

# Data configuration
data:
  csv_path: "/Users/gabrieleconte/Developer/pokemon-sprite-generator/data/text_description_concat.csv"
  image_dir: "/Users/gabrieleconte/Developer/pokemon-sprite-generator/data/small_images"
  batch_size: 8  # Reduced for better GPU memory usage
  image_size: 215
  num_workers: 4  # Reduced for stability
  pin_memory: true
  val_split: 0.15
  test_split: 0.05

# Training configuration
training:
  # Stage-specific epochs (optimized for practical training)
  vae_epochs: 50     # VAE foundation training
  diffusion_epochs: 80  # U-Net training  
  final_epochs: 60    # Final joint training
  
  # KL annealing for VAE (prevents posterior collapse)
  kl_anneal_start: 0
  kl_anneal_end: 20   # Shorter annealing period
  kl_weight_start: 0.0
  kl_weight_end: 0.1  # Lower final KL weight for better generation
  free_bits: 0.5
  
  # CLIP loss weight
  clip_weight: 0.3    # Increased for better text-image alignment
  
  # Training phases for final stage
  phase1_epochs: 30   # Text encoder fine-tuning
  
  # Logging and checkpoints
  log_every: 25       # More frequent logging
  save_every: 10
  sample_every: 5

# Optimization
optimization:
  optimizer: "adamw"
  learning_rate: 0.0001      # Conservative learning rate
  text_encoder_lr: 0.00005   # Lower for fine-tuning
  vae_decoder_lr: 0.00005    # Lower for fine-tuning
  beta1: 0.9                 # Standard Adam betas
  beta2: 0.999
  weight_decay: 0.0001       # Regularization
  scheduler: "cosine"
  max_grad_norm: 1.0         # Gradient clipping for stability

# Device
device: "mps"