# Optimized Configuration for Pokemon Sprite Generator
# C# Optimization (tuned for stability and fine-tuning pre-trained model)

# Experiment settings
experiment_dir: "/Users/gabrieleconte/Developer/pokemon-sprite-generator/experiments"

# Model configuration
model:
  # Text encoder - using BERT-base for 768-dimensional embeddings
  bert_model: "google-bert/bert-base-uncased"
  text_embedding_dim: 768     # BERT-base hidden dimension
  
  # VAE/Latent space
  latent_dim: 8  # 8 channels for 27x27 latent space
  
  # Pre-trained Stable Diffusion U-Net configuration
  pretrained_model_name: "runwayml/stable-diffusion-v1-5"  # Use pre-trained SD 1.5
  cross_attention_dim: 768    # Standard SD cross-attention dimension
  attention_head_dim: 8       # Reduced for efficiency
  use_flash_attention: true   # Enable for performance
  freeze_encoder: true       # Keep encoder trainable for cross-attention
  freeze_decoder: true       # Keep decoder trainable for cross-attention
  
  # Diffusion scheduler parameters
  num_timesteps: 1000
  beta_start: 0.0001
  beta_end: 0.02

# Data configuration
data:
  csv_path: "/Users/gabrieleconte/Developer/pokemon-sprite-generator/data/text_description_concat.csv"
  image_dir: "/Users/gabrieleconte/Developer/pokemon-sprite-generator/data/small_images"
  batch_size: 6               # Reduced to prevent memory swapping
  image_size: 215
  num_workers: 4              # Reduced to save memory
  pin_memory: false           # Disabled for MPS compatibility
  val_split: 0.15
  test_split: 0.05

# Training configuration
training:
  # Stage-specific epochs (optimized for fine-tuning)
  vae_epochs: 50              # Keep VAE training the same
  diffusion_epochs: 50        # Reduced for fine-tuning (was 400)
  final_epochs: 30            # Reduced from 60
  
  # VAE KL annealing (proven effective settings)
  kl_anneal_start: 0
  kl_anneal_end: 15           # Faster annealing
  kl_weight_start: 0.0
  kl_weight_end: 0.1          # Lower final weight for better generation
  free_bits: 0.5              # Prevents posterior collapse
  
  # Logging and checkpoints (more frequent for debugging)
  log_every: 20
  save_every: 10
  sample_every: 5

# Optimization (tuned for stability and fine-tuning pre-trained models with BERT-large)
optimization:
  optimizer: "adamw"
  learning_rate: 0.0002        # Conservative rate for U-Net fine-tuning
  text_encoder_lr: 0.00005     # Very conservative rate for BERT-large fine-tuning
  weight_decay: 0.01           # Moderate regularization
  
  # Training stability and memory optimization
  max_grad_norm: 1.0           # More permissive gradient clipping (was 0.5)
  use_mixed_precision: false   # Disable for MPS stability
  
  # Scheduler (disabled for stable training)
  scheduler: "constant"         # Use constant LR instead of cosineoptimized for diffusers-based training
  
# Device
device: "mps"
